import pandas as pd
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
import os
import time

class EconomicTermRAG:
    def __init__(self, google_api_key):
        clean_key = google_api_key.strip()
        os.environ["GOOGLE_API_KEY"] = clean_key
        
        self.embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
        self.vector_db = None
        self.DB_PATH = "faiss_db_cache"

        if os.path.exists(self.DB_PATH):
            self._load_from_disk()
        else:
            self._build_from_excel()

    def _load_from_disk(self):
        print("ğŸ’¾ ì €ì¥ëœ ë°ì´í„°ë² ì´ìŠ¤ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì¤‘...")
        self.vector_db = FAISS.load_local(
            self.DB_PATH, 
            self.embeddings, 
            allow_dangerous_deserialization=True
        )

    def _build_from_excel(self):
        print("ğŸ“š ì—‘ì…€ ë°ì´í„°ë¥¼ ì½ì–´ì˜¤ëŠ” ì¤‘ì…ë‹ˆë‹¤...")
        documents = []

        try:
            # íŒŒì¼ëª…ì€ ì„œí˜„ë‹˜ì˜ ì‹¤ì œ íŒŒì¼ëª…ê³¼ ì¼ì¹˜í•´ì•¼ í•©ë‹ˆë‹¤
            df1 = pd.read_excel("data/20251205_ì‹œì‚¬ê²½ì œìš©ì–´ì‚¬ì „.xlsx")
            for _, row in df1.iterrows():
                term = str(row['ìš©ì–´'])
                desc = str(row['ì„¤ëª…'])
                if term != 'nan' and desc != 'nan':
                    content = f"ìš©ì–´: {term}\nì„¤ëª…: {desc}"
                    documents.append(Document(page_content=content, metadata={"term": term}))
        except Exception as e:
            print(f"âš ï¸ ì—‘ì…€ ë¡œë“œ ì‹¤íŒ¨: {e}")

        try:
            df2 = pd.read_excel("data/ê¸°íšì¬ì •ë¶€_ê²½ì œìš©ì–´_20240905.xlsx")
            for _, row in df2.iterrows():
                term = str(row['ê²½ì œìš©ì–´'])
                desc = str(row['ìš©ì–´ì„¤ëª…'])
                if term != 'nan' and desc != 'nan':
                    content = f"ìš©ì–´: {term}\nì„¤ëª…: {desc}"
                    documents.append(Document(page_content=content, metadata={"term": term}))
        except:
            pass

        if documents:
            print(f"ğŸ§  ì´ {len(documents)}ê°œì˜ ìš©ì–´ í•™ìŠµ ì‹œì‘ (ì„œë²„ ë³´í˜¸ ëª¨ë“œ)")
            batch_size = 50
            self.vector_db = FAISS.from_documents(documents[:batch_size], self.embeddings)
            time.sleep(1)

            for i in range(batch_size, len(documents), batch_size):
                batch = documents[i : i + batch_size]
                # print(f"   ğŸš€ í•™ìŠµ ì¤‘... ({i}/{len(documents)})")
                for attempt in range(3):
                    try:
                        self.vector_db.add_documents(batch)
                        time.sleep(1.5)
                        break
                    except:
                        time.sleep(5)
            
            self.vector_db.save_local(self.DB_PATH)
            print("âœ… í•™ìŠµ ë° ì €ì¥ ì™„ë£Œ!")
        else:
            print("âŒ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

    def search_terms(self, query_text, k=3):
        """
        ìœ ì‚¬ë„ ì ìˆ˜(Score)ë¥¼ í™•ì¸í•˜ì—¬ ì—‰ëš±í•œ ê²°ê³¼ í•„í„°ë§
        """
        if not self.vector_db:
            return []
        
        # ì ìˆ˜ì™€ í•¨ê»˜ ê²€ìƒ‰ (ë‚®ì„ìˆ˜ë¡ ì •í™•í•¨, 0.5 ì´ìƒì´ë©´ ì—‰ëš±í•œ ê²ƒ)
        results_with_scores = self.vector_db.similarity_search_with_score(query_text, k=k)
        
        terms_info = []
        # ê¸°ì¤€ì  (ì´ ì ìˆ˜ë³´ë‹¤ ë†’ìœ¼ë©´ ë²„ë¦¼)
        THRESHOLD = 0.48 
        
        print(f"\n   [ê²€ìƒ‰ ë””ë²„ê¹…] '{query_text}' ê²°ê³¼:")
        for doc, score in results_with_scores:
            if score < THRESHOLD:
                print(f"      âœ… ì±„íƒ (ì ìˆ˜: {score:.3f}) - {doc.metadata.get('term')}")
                terms_info.append(doc.page_content)
            else:
                print(f"      ğŸ—‘ï¸ íƒˆë½ (ì ìˆ˜: {score:.3f} - ë„ˆë¬´ ë‹¤ë¦„) - {doc.metadata.get('term')}")
        
        return terms_info